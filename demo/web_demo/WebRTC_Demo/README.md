# WebRTC Real-Time Video Interaction Demo

A full-duplex real-time video interaction solution based on WebRTC, enabling seamless streaming input/output with high responsiveness and low latency.

ðŸ“– [ä¸­æ–‡ç‰ˆæœ¬](./README_zh.md)

## Overview

This demo implements a **full-duplex real-time video interaction** solution using WebRTC technology. It fills a significant gap in the open-source community by providing a **streaming duplex conversation** capability that was previously unavailable.

> [!IMPORTANT]
> **Note on Audio Quality**: The current local demo implementation has a known issue that may cause slight "electric noise", leading to lower audio quality than the online demo. We are actively working on this and expect a fix within the next few days.

## Prerequisites

### 1. Install Docker Desktop (macOS)

```bash
# Install via Homebrew
brew install --cask docker

# Or download from: https://www.docker.com/products/docker-desktop

# Verify installation
docker --version
```

### 2. Build llamacpp-omni Inference Service

```bash
# Clone and enter the project directory
git clone https://github.com/tc-mb/llama.cpp-omni.git
cd llama.cpp-omni

# Build (Metal acceleration enabled by default on macOS)
cmake -B build -DCMAKE_BUILD_TYPE=Release
cmake --build build --target llama-server -j

# Verify build
ls -la build/bin/llama-server
```

### 3. Prepare GGUF Model Files

Download and organize the model files with the following structure:

```
<MODEL_DIR>/
â”œâ”€â”€ MiniCPM-o-4_5-Q4_K_M.gguf        # LLM main model (~5GB)
â”œâ”€â”€ audio/                            # Audio encoder
â”‚   â””â”€â”€ MiniCPM-o-4_5-audio-F16.gguf
â”œâ”€â”€ vision/                           # Vision encoder
â”‚   â””â”€â”€ MiniCPM-o-4_5-vision-F16.gguf
â”œâ”€â”€ tts/                              # TTS model
â”‚   â”œâ”€â”€ MiniCPM-o-4_5-tts-F16.gguf
â”‚   â””â”€â”€ MiniCPM-o-4_5-projector-F16.gguf
â””â”€â”€ token2wav-gguf/                   # Token2Wav model
    â”œâ”€â”€ encoder.gguf
    â”œâ”€â”€ flow_matching.gguf
    â”œâ”€â”€ flow_extra.gguf
    â”œâ”€â”€ hifigan2.gguf
    â””â”€â”€ prompt_cache.gguf
```

## Quick Start

We provide a pre-built Docker image for quick deployment and experience. The Docker image includes all necessary dependencies and configurations.

### macOS (Apple Silicon)

**Requirements**: Apple Silicon Mac (M1/M2/M3/M4), **M4 recommended** for optimal performance.

Download the Docker image for macOS:

ðŸ“¦ [Download Docker Image (macOS)](https://drive.google.com/file/d/1vOi2T_l-MED7-q7fW-G1GHiHoDDcObxJ/view?usp=sharing)

### Deployment Steps

#### Step 1: Extract and Load Docker Images

```bash
# Extract the package
unzip omni_docker.zip
cd omni_docker

# Open Docker
open -a Docker

# Load Docker images
docker load -i o45-frontend.tar
docker load -i omini_backend_code/omni_backend.tar
```

#### Step 2: Install Python Dependencies

```bash
# Install required Python dependencies for the inference service
pip install -r cpp_server/requirements.txt
```

#### Step 3: One-Click Deployment (Recommended)

```bash
# Run the deployment script with required paths
./deploy_all.sh \
    --cpp-dir /path/to/llama.cpp-omni \
    --model-dir /path/to/gguf

# For duplex mode
./deploy_all.sh \
    --cpp-dir /path/to/llama.cpp-omni \
    --model-dir /path/to/gguf \
    --duplex
```

The script automatically:
- Checks Docker environment
- Updates LiveKit configuration with local IP
- Starts Docker services (frontend, backend, LiveKit, Redis)
- Installs Python dependencies
- Starts C++ inference service
- Registers inference service to backend

#### Step 4: Access the Web Interface

```bash
# Open the frontend in browser
open http://localhost:3000
```

### Service Ports

| Service | Port | Description |
|---------|------|-------------|
| Frontend | 3000 | Web UI |
| Backend | 8021 | Backend API |
| LiveKit | 7880 | Real-time communication |
| Inference | 9060 | Python HTTP API |

> More platform support (Linux, Windows) coming soon.

## Key Features

### ðŸ”„ Full-Duplex Communication
- Simultaneous bidirectional audio and video streaming
- Natural conversation flow without turn-taking delays

### âš¡ High Responsiveness & Low Latency
- Streaming input/output for real-time interactions
- Optimized for minimal end-to-end latency
- Immediate feedback during conversations

### ðŸš€ Native llamacpp-omni Support
- Seamlessly integrates with [llamacpp-omni](https://github.com/OpenBMB/llama.cpp/tree/minicpm-omni) as the inference backend
- Quick deployment and easy setup
- Efficient resource utilization

### ðŸŽ¯ MiniCPM-o 4.5 Experience
- Rapidly experience the full capabilities of MiniCPM-o 4.5
- Real-time multimodal understanding and generation
- Voice and video interaction in one unified interface

## Technical Highlights

- **WebRTC Protocol**: Industry-standard real-time communication
- **Streaming Architecture**: Continuous data flow for smooth interactions
- **Duplex Design**: Fills the gap in open-source streaming duplex conversation solutions

## Coming Soon

> ðŸš§ **We are currently organizing and refining the code. The complete source code will be open-sourced within the next few days. Stay tuned!**

## Related Resources

- [MiniCPM-o 4.5 Model](https://huggingface.co/openbmb/MiniCPM-o-4_5)
- [llamacpp-omni Backend](https://github.com/OpenBMB/llama.cpp/tree/minicpm-omni)
